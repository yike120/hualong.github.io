<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/bitbug_favicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/bitbug_favicon16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://xiaoliaozi.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="什么是线性回归定义： 线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。  特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https://xiaoliaozi.com/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="小廖子的博客">
<meta property="og:description" content="什么是线性回归定义： 线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。  特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223100938.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101022.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101102.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101110.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101752.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101812.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101843.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101854.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223103504.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223103515.gif">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215800.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215850.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215953.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220026.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220139.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220335.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220422.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220214.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220540.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220637.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220738.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220745.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220755.png">
<meta property="article:published_time" content="2020-01-08T06:23:50.000Z">
<meta property="article:modified_time" content="2020-02-23T14:08:35.366Z">
<meta property="article:author" content="小廖子">
<meta property="article:tag" content="机器学习算法">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="监督学习算法">
<meta property="article:tag" content="梯度下降">
<meta property="article:tag" content="正规方程">
<meta property="article:tag" content="正则化">
<meta property="article:tag" content="岭回归">
<meta property="article:tag" content="模型保存和加载">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223100938.png">

<link rel="canonical" href="https://xiaoliaozi.com/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>线性回归 | 小廖子的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="小廖子的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小廖子的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">好记性不如记笔记</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xiaoliaozi.com/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avter.jpg">
      <meta itemprop="name" content="小廖子">
      <meta itemprop="description" content="学无止境">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小廖子的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          线性回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-01-08 14:23:50" itemprop="dateCreated datePublished" datetime="2020-01-08T14:23:50+08:00">2020-01-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-23 22:08:35" itemprop="dateModified" datetime="2020-02-23T22:08:35+08:00">2020-02-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h4><p><strong>定义：</strong> 线性回归(Linear regression)是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。 </p>
<p>特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归 </p>
<a id="more"></a>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223100938.png"/></p>
<p>在机器学习中<strong>特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。 </p>
<h4 id="线性回归的特征与目标的关系"><a href="#线性回归的特征与目标的关系" class="headerlink" title="线性回归的特征与目标的关系"></a>线性回归的特征与目标的关系</h4><p> 线性回归当中主要有两种模型，<strong>一种是线性关系，另一种是非线性关系。</strong>在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。 </p>
<ul>
<li><p>线性关系</p>
<ul>
<li><p>单变量关系：</p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101022.png"/></p>
</li>
</ul>
</li>
<li><p>多变量线性关系 </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101102.png"/></p>
<p>注释： 单特征与目标值的关系呈直线关系，两个特征与目标值呈现平面的关系  </p>
</li>
<li><p>非线性关系 </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101110.png"/></p>
</li>
</ul>
<h4 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.linear_model</span><span class="selector-class">.LinearRegression</span>()</span><br><span class="line">	# 基于正规方程</span><br><span class="line">	<span class="selector-tag">LinearRegression</span><span class="selector-class">.coef_</span>：回归系数</span><br></pre></td></tr></table></figure>
<p><strong>小案例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line"><span class="comment"># 已知学生平时成绩和期末成绩，预测最终成绩</span></span><br><span class="line">x = [[<span class="number">80</span>, <span class="number">86</span>],[<span class="number">82</span>, <span class="number">80</span>],[<span class="number">85</span>, <span class="number">78</span>],[<span class="number">90</span>, <span class="number">90</span>],</span><br><span class="line">     [<span class="number">86</span>, <span class="number">82</span>],[<span class="number">82</span>, <span class="number">90</span>],[<span class="number">78</span>, <span class="number">80</span>],[<span class="number">92</span>, <span class="number">94</span>]]</span><br><span class="line">y = [<span class="number">84.2</span>, <span class="number">80.6</span>, <span class="number">80.1</span>, <span class="number">90</span>, <span class="number">83.2</span>, <span class="number">87.6</span>, <span class="number">79.4</span>, <span class="number">93.4</span>]</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x,y)</span><br><span class="line">print(estimator.coef_)</span><br><span class="line">print(estimator.predict([[<span class="number">100</span>, <span class="number">80</span>]]))</span><br></pre></td></tr></table></figure>
<h4 id="线性回归的损失和优化"><a href="#线性回归的损失和优化" class="headerlink" title="线性回归的损失和优化"></a>线性回归的损失和优化</h4><ol>
<li><p><strong>损失函数</strong></p>
<p>总损失定义为：</p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101752.png"/></p>
<ul>
<li>$y_i$为第i个训练样本的真实值</li>
<li>$h(x_i)$为第i个训练样本特征值组合预测函数</li>
<li>又称<strong>最小二乘法</strong></li>
</ul>
<p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！</p>
</li>
<li><p><strong>优化算法</strong></p>
<p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p>
<ul>
<li>线性回归经常使用的两种优化算法<ul>
<li>正规方程</li>
<li>梯度下降法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101812.png"/></p>
<p> 理解：X为特征值矩阵，y为目标值矩阵。<strong>直接求到最好的结果</strong></p>
<p><strong>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</strong></p>
<p><strong>正规方程的推导：</strong></p>
<p> 把该损失函数转换成矩阵写法： </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101843.png"/></p>
<p>其中$y$是真实值矩阵，$X$是特征值矩阵，$w$是权重矩阵</p>
<p>对其求解关于$w$的最小值，起止$y$，$X$ 均已知二次函数直接求导，导数为零的位置，即为最小值。</p>
<p>对损失函数求导：</p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223101854.png"/></p>
<h4 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h4><p><strong>梯度</strong>：梯度是微积分中一个很重要的概念，<strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong> <strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；</strong></p>
<p> <strong>梯度下降（Gradient Descent）公式</strong></p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223103504.png"/></p>
<p>  <strong>α是什么含义</strong> </p>
<p> α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点。</p>
<p> <strong>所以有了梯度下降这样一个优化算法，回归就有了”自动学习”的能力</strong> </p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223103515.gif"/> </p>
<h4 id="梯度下降和正规方程的对比"><a href="#梯度下降和正规方程的对比" class="headerlink" title="梯度下降和正规方程的对比"></a>梯度下降和正规方程的对比</h4><div class="table-container">
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率</td>
<td>不需要</td>
</tr>
<tr>
<td>需要迭代求解</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>特征数量较大可以使用</td>
<td>需要计算方程，时间复杂度高O(n3)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="算法选择依据"><a href="#算法选择依据" class="headerlink" title="算法选择依据"></a>算法选择依据</h4><ul>
<li>小规模数据：<ul>
<li>正规方程：<strong>LinearRegression(不能解决拟合问题)</strong></li>
<li>岭回归</li>
</ul>
</li>
<li>大规模数据：<ul>
<li>梯度下降法：<strong>SGDRegressor</strong></li>
</ul>
</li>
</ul>
<h4 id="常见的梯度下降算法"><a href="#常见的梯度下降算法" class="headerlink" title="常见的梯度下降算法"></a>常见的梯度下降算法</h4><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">全梯度下降算法(<span class="literal">Full</span> gradient descent）,</span><br><span class="line">随机梯度下降算法（Stochastic gradient descent）,</span><br><span class="line">小批量梯度下降算法（Mini<span class="params">-batch</span> gradient descent）,</span><br><span class="line">随机平均梯度下降算法（Stochastic <span class="keyword">average</span> gradient descent）</span><br></pre></td></tr></table></figure>
<h5 id="全梯度下降算法（FG）"><a href="#全梯度下降算法（FG）" class="headerlink" title="全梯度下降算法（FG）"></a>全梯度下降算法（FG）</h5><ol>
<li>计算训练集所有样本误差，对其求和再取平均值作为目标函数。</li>
<li>权重向量沿其梯度相反的方向移动，从而使当前目标函数减少得最多。</li>
<li>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</li>
<li>批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215800.png"/></p>
<h5 id="随机梯度下降算法（SG）"><a href="#随机梯度下降算法（SG）" class="headerlink" title="随机梯度下降算法（SG）"></a>随机梯度下降算法（SG）</h5><p>由于FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解，因此提出了随机梯度下降算法。</p>
<p>其每轮计算的目标函数不再是全体样本误差，而仅是单个样本误差，即<strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></p>
<p>此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。 但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。 </p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215850.png"/> </p>
<h5 id="小批量梯度下降算法（mini-batch）"><a href="#小批量梯度下降算法（mini-batch）" class="headerlink" title="小批量梯度下降算法（mini-batch）"></a>小批量梯度下降算法（mini-batch）</h5><p>小批量梯度下降算法是FG和SG的折中方案,在一定程度上兼顾了以上两种方法的优点。</p>
<p><strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。</strong></p>
<p>被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。</p>
<p>特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为</p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223215953.png"/></p>
<h5 id="随机平均梯度下降算法（SAG）"><a href="#随机平均梯度下降算法（SAG）" class="headerlink" title="随机平均梯度下降算法（SAG）"></a>随机平均梯度下降算法（SAG）</h5><p>在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。</p>
<p><strong>随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</strong></p>
<p>如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多。</p>
<h4 id="梯度下降法算法比较"><a href="#梯度下降法算法比较" class="headerlink" title="梯度下降法算法比较"></a>梯度下降法算法比较</h4><ul>
<li>全梯度下降算法(Full gradient descent）,</li>
<li>随机梯度下降算法（Stochastic gradient descent）,</li>
<li>小批量梯度下降算法（Mini-batch gradient descent）,</li>
<li>随机平均梯度下降算法（Stochastic average gradient descent）</li>
</ul>
<ol>
<li><p>FG方法由于它每轮更新都要使用全体数据集，故花费的时间成本最多，内存存储最大。</p>
</li>
<li><p>SAG在训练初期表现不佳，优化速度较慢。这是因为我们常将初始梯度设为0，而SAG每轮梯度更新都结合了上一轮梯度值。</p>
</li>
<li>综合考虑迭代次数和运行时间，SG表现性能都很好，能在训练初期快速摆脱初始梯度值，快速将平均损失函数降到很低。但要注意，在使用SG方法时要慎重选择步长，否则容易错过最优解。 </li>
<li>mini-batch结合了SG的“胆大”和FG的“心细”，从6幅图像来看，它的表现也正好居于SG和FG二者之间。在目前的机器学习领域，mini-batch是使用最多的梯度下降算法，正是因为它避开了FG运算效率低成本大和SG收敛效果不稳定的缺点。 </li>
</ol>
<h4 id="基于梯度下降法API"><a href="#基于梯度下降法API" class="headerlink" title="基于梯度下降法API"></a>基于梯度下降法API</h4><p>正规方程</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.<span class="constructor">LinearRegression(<span class="params">fit_intercept</span>=True)</span></span><br><span class="line">	通过正规方程优化</span><br><span class="line">	参数</span><br><span class="line">		fit_intercept：是否计算偏置</span><br><span class="line">	属性</span><br><span class="line">		<span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>coef_：回归系数</span><br><span class="line">		<span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>intercept_：偏置</span><br></pre></td></tr></table></figure>
<p><strong>随机梯度下降法</strong></p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.SGDRegressor(<span class="attribute">loss</span>=<span class="string">"squared_loss"</span>, <span class="attribute">fit_intercept</span>=<span class="literal">True</span>, learning_rate =<span class="string">'invscaling'</span>, <span class="attribute">eta0</span>=0.01)</span><br><span class="line">	SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。</span><br><span class="line">	参数：</span><br><span class="line">	loss:损失类型</span><br><span class="line">		<span class="attribute">loss</span>=”squared_loss”: 普通最小二乘法</span><br><span class="line">	fit_intercept：是否计算偏置</span><br><span class="line">	learning_rate : string, optional</span><br><span class="line">		学习率填充</span><br><span class="line">		<span class="string">'constant'</span>: eta = eta0</span><br><span class="line">		<span class="string">'optimal'</span>: eta = 1.0 / (alpha * (t + t0) [default]</span><br><span class="line">		<span class="string">'invscaling'</span>: eta = eta0 / pow(t, power_t)</span><br><span class="line">			<span class="attribute">power_t</span>=0.25:存在父类当中</span><br><span class="line">	对于一个常数值的学习率来说，可以使用<span class="attribute">learning_rate</span>=’constant’ ，并使用eta0来指定学习率。</span><br><span class="line">	属性：</span><br><span class="line">		SGDRegressor.coef_：回归系数</span><br><span class="line">		SGDRegressor.intercept_：偏置</span><br></pre></td></tr></table></figure>
<h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ol>
<li><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><ul>
<li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li>
<li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li>
</ul>
</li>
<li><h5 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h5><ul>
<li><p>欠拟合原因以及解决办法</p>
<p>原因：学习到数据的特征过少</p>
<p>解决办法：继续学习</p>
<p><strong>1）添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。</p>
<p><strong>2）添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</p>
</li>
<li><p>过拟合原因以及解决办法</p>
<p>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</p>
<p>解决办法：</p>
<p>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</p>
<p>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p>
<p><strong>3）正则化</strong></p>
<p>4）减少特征维度，防止<strong>维灾难</strong></p>
</li>
</ul>
</li>
<li><p>练一练 ：尝试不看后面的字回答出来是解决过拟合的措施，还是解决欠拟合的措施？</p>
<p><strong>过拟合和欠拟合的解决方案</strong></p>
<ul>
<li>获取更多样本 过 </li>
<li>减少特征个数 过 </li>
<li>增加特征个数 欠 </li>
<li>使用高次项的特征 欠 </li>
<li>增加正则化的惩罚系数 过 </li>
<li>减少正则化的惩罚系数 欠   </li>
</ul>
<p><strong>解决过拟合和欠拟合的措施</strong> </p>
<p>过拟合解决方案： </p>
<ul>
<li>获取更多样本</li>
<li>减少特征个数 </li>
<li>增加正则化的惩罚系数</li>
</ul>
<p>欠拟合解决方案:</p>
<ul>
<li>增加特征个数 </li>
<li>使用高次项的特征 </li>
<li>减少正则化的惩罚系数 </li>
</ul>
</li>
</ol>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p> 在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征 。</p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220026.png"/></p>
<p>  <strong>如何解决？</strong> </p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220139.png"/></p>
<p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p>
<p>注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果</p>
<h4 id="正则化类别"><a href="#正则化类别" class="headerlink" title="正则化类别"></a>正则化类别</h4><ul>
<li>L2正则化<ul>
<li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li>
<li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li>
<li>Ridge回归</li>
</ul>
</li>
<li>L1正则化<ul>
<li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li>
<li>LASSO回归</li>
</ul>
</li>
</ul>
<h4 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h4><h5 id="岭回归（Ridge-Regression）"><a href="#岭回归（Ridge-Regression）" class="headerlink" title="岭回归（Ridge Regression）"></a>岭回归（Ridge Regression）</h5><p>岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）: </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220335.png"/> </p>
<p>以达到在拟合数据的同时，使模型权重尽可能小的目的,岭回归代价函数: </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220422.png"/> </p>
<p>α=0：岭回归退化为线性回归。 </p>
<h5 id="Lasso-Regression-Lasso-回归"><a href="#Lasso-Regression-Lasso-回归" class="headerlink" title="Lasso Regression(Lasso 回归)"></a>Lasso Regression(Lasso 回归)</h5><p>Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的ℓ1范数。</p>
<p>Lasso回归的代价函数 ：</p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220214.png"/></p>
<p>【注意 】</p>
<ul>
<li>Lasso Regression 的代价函数在 θi=0处是不可导的.</li>
<li>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，如下式</li>
<li><p>Lasso Regression 的次梯度向量</p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220540.png"/></p>
</li>
</ul>
<p>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</p>
<p>例如：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。</p>
<p>也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p>
<h5 id="Elastic-Net-弹性网络"><a href="#Elastic-Net-弹性网络" class="headerlink" title="Elastic Net (弹性网络)"></a>Elastic Net (弹性网络)</h5><p>弹性网络在岭回归和Lasso回归中进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制， 是前两个内容的综合 ：</p>
<ul>
<li>r=0：弹性网络变为岭回归</li>
<li>r=1：弹性网络便为Lasso回归</li>
</ul>
<p>弹性网络的代价函数 ：</p>
<p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220637.png"/></p>
<ul>
<li><p><strong>Early Stopping</strong></p>
<p>Early Stopping 也是正则化迭代学习的方法之一。</p>
<p>其做法为：在验证错误率达到最小值的时候停止训练。</p>
</li>
<li><p><strong>选择正则化方法</strong></p>
<p> 一般来说，我们应避免使用<strong>朴素线性回归</strong>，而应对模型进行一定的正则化处理 。</p>
<ul>
<li>常用：岭回归</li>
<li>假设只有少部分特征是有用的：<ul>
<li>弹性网络</li>
<li>Lasso</li>
<li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定。</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="岭回归API"><a href="#岭回归API" class="headerlink" title="岭回归API"></a>岭回归API</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, ElasticNet, Lasso</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Ridge</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.Ridge(alpha=<span class="number">1.0</span>, fit_intercept=<span class="literal">True</span>,solver=<span class="string">"auto"</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">	具有l2正则化的线性回归</span><br><span class="line">	alpha:正则化力度，也叫 λ</span><br><span class="line">		λ取值：<span class="number">0</span>~<span class="number">1</span> <span class="number">1</span>~<span class="number">10</span></span><br><span class="line">	solver:会根据数据自动选择优化方法</span><br><span class="line">		sag:如果数据集、特征都比较大，选择该随机梯度下降优化</span><br><span class="line">	normalize:数据是否进行标准化</span><br><span class="line">		normalize=<span class="literal">False</span>:可以在fit之前调用preprocessing.StandardScaler标准化数据</span><br><span class="line">	Ridge.coef_:回归权重</span><br><span class="line">	Ridge.intercept_:回归偏置</span><br><span class="line">Ridge方法相当于SGDRegressor(penalty=<span class="string">'l2'</span>, loss=<span class="string">"squared_loss"</span>),</span><br><span class="line">只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</span><br><span class="line">sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)</span><br><span class="line">	具有l2正则化的线性回归，可以进行交叉验证</span><br><span class="line">	coef_:回归系数</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="案例-波士顿房价预测"><a href="#案例-波士顿房价预测" class="headerlink" title="案例-波士顿房价预测"></a>案例-波士顿房价预测</h4><p><strong>数据介绍：</strong></p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220738.png"/></p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220745.png"/></p>
<p> <strong>案例分析：</strong></p>
<p>回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。</p>
<ul>
<li>数据分割与标准化处理</li>
<li>回归预测</li>
<li>线性回归的算法效果评估</li>
</ul>
<p><strong>回归性能评估：</strong></p>
<p> 均方误差(Mean Squared Error)MSE)评价机制： </p>
<p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200223220755.png"/></p>
<p>$ y_i$为预测值，$\overline{y}$ 为真实值 </p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred)</span><br><span class="line">	均方误差回归损失</span><br><span class="line"><span class="symbol">	y_true:</span>真实值</span><br><span class="line"><span class="symbol">	y_pred:</span>预测值</span><br><span class="line"><span class="symbol">	return:</span>浮点数结果</span><br></pre></td></tr></table></figure>
<p> <strong>代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor, Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：正规方程"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = LinearRegression()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：梯度下降法"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># estimator = SGDRegressor(max_iter=1000, learning_rate="constant", eta0=1)</span></span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model3</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：岭回归"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    estimator = RidgeCV()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    linear_model1()</span><br><span class="line">    linear_model2()</span><br><span class="line">    linear_model3()</span><br></pre></td></tr></table></figure>
<h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><ul>
<li><h5 id="sklearn模型的保存和加载API"><a href="#sklearn模型的保存和加载API" class="headerlink" title="sklearn模型的保存和加载API"></a>sklearn模型的保存和加载API</h5></li>
<li><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_load</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""模型保存和加载"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">22</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># 4.1 模型训练</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    <span class="comment"># estimator = RidgeCV()</span></span><br><span class="line">    <span class="comment"># estimator.fit(x_train, y_train)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 4.2 模型保存</span></span><br><span class="line">    <span class="comment"># joblib.dump(estimator, "./data/test.pkl")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.3 模型加载</span></span><br><span class="line">    estimator = joblib.load(<span class="string">"./data/test.pkl"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line">dump_load()</span><br></pre></td></tr></table></figure>
</li>
<li><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul>
<li>1.保存文件，后缀名是**.pkl</li>
<li>2.加载模型是需要通过一个变量进行承接</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
            <!---<a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" rel="tag"># 机器学习算法</a>--->
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" rel="tag">机器学习算法</a>
            <!---<a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>--->
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
            <!---<a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" rel="tag"># 监督学习算法</a>--->
            <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" rel="tag">监督学习算法</a>
            <!---<a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag"># 梯度下降</a>--->
            <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">梯度下降</a>
            <!---<a href="/tags/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/" rel="tag"># 正规方程</a>--->
            <a href="/tags/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/" rel="tag">正规方程</a>
            <!---<a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag"># 正则化</a>--->
            <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag">正则化</a>
            <!---<a href="/tags/%E5%B2%AD%E5%9B%9E%E5%BD%92/" rel="tag"># 岭回归</a>--->
            <a href="/tags/%E5%B2%AD%E5%9B%9E%E5%BD%92/" rel="tag">岭回归</a>
            <!---<a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/" rel="tag"># 模型保存和加载</a>--->
            <a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/" rel="tag">模型保存和加载</a>
            <br>
          <div class="fa fa-tag">本文结束</div>
          <br/>
          </div>
        

        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/01/07/knn%E7%AE%97%E6%B3%95/" rel="prev" title="knn算法">
      <i class="fa fa-chevron-left"></i> knn算法
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="next" title="逻辑回归">
      逻辑回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是线性回归"><span class="nav-number">1.</span> <span class="nav-text">什么是线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归的特征与目标的关系"><span class="nav-number">2.</span> <span class="nav-text">线性回归的特征与目标的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归API"><span class="nav-number">3.</span> <span class="nav-text">线性回归API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归的损失和优化"><span class="nav-number">4.</span> <span class="nav-text">线性回归的损失和优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正规方程"><span class="nav-number">5.</span> <span class="nav-text">正规方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降-Gradient-Descent"><span class="nav-number">6.</span> <span class="nav-text">梯度下降(Gradient Descent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降和正规方程的对比"><span class="nav-number">7.</span> <span class="nav-text">梯度下降和正规方程的对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#算法选择依据"><span class="nav-number">8.</span> <span class="nav-text">算法选择依据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的梯度下降算法"><span class="nav-number">9.</span> <span class="nav-text">常见的梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#全梯度下降算法（FG）"><span class="nav-number">9.1.</span> <span class="nav-text">全梯度下降算法（FG）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机梯度下降算法（SG）"><span class="nav-number">9.2.</span> <span class="nav-text">随机梯度下降算法（SG）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小批量梯度下降算法（mini-batch）"><span class="nav-number">9.3.</span> <span class="nav-text">小批量梯度下降算法（mini-batch）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机平均梯度下降算法（SAG）"><span class="nav-number">9.4.</span> <span class="nav-text">随机平均梯度下降算法（SAG）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法算法比较"><span class="nav-number">10.</span> <span class="nav-text">梯度下降法算法比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于梯度下降法API"><span class="nav-number">11.</span> <span class="nav-text">基于梯度下降法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欠拟合和过拟合"><span class="nav-number">12.</span> <span class="nav-text">欠拟合和过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#定义"><span class="nav-number">12.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#原因以及解决办法"><span class="nav-number">12.2.</span> <span class="nav-text">原因以及解决办法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正则化"><span class="nav-number">13.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正则化类别"><span class="nav-number">14.</span> <span class="nav-text">正则化类别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正则化线性模型"><span class="nav-number">15.</span> <span class="nav-text">正则化线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#岭回归（Ridge-Regression）"><span class="nav-number">15.1.</span> <span class="nav-text">岭回归（Ridge Regression）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Lasso-Regression-Lasso-回归"><span class="nav-number">15.2.</span> <span class="nav-text">Lasso Regression(Lasso 回归)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Elastic-Net-弹性网络"><span class="nav-number">15.3.</span> <span class="nav-text">Elastic Net (弹性网络)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#岭回归API"><span class="nav-number">15.4.</span> <span class="nav-text">岭回归API</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例-波士顿房价预测"><span class="nav-number">16.</span> <span class="nav-text">案例-波士顿房价预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型的保存和加载"><span class="nav-number">17.</span> <span class="nav-text">模型的保存和加载</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#sklearn模型的保存和加载API"><span class="nav-number">17.1.</span> <span class="nav-text">sklearn模型的保存和加载API</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#案例"><span class="nav-number">17.2.</span> <span class="nav-text">案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#注意："><span class="nav-number">17.3.</span> <span class="nav-text">注意：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <a href="/"><img class="site-author-image" itemprop="image" alt="小廖子"
      src="/images/avter.jpg">
    </a>
  <p class="site-author-name" itemprop="name">小廖子</p>
  <div class="site-description" itemprop="description">学无止境</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xlztongxue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xlztongxue" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xlz_tongxue@163.com" title="E-Mail → mailto:xlz_tongxue@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_45439324" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_45439324" rel="noopener" target="_blank"><i class="fa fa-fw fa-copyright"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
  </div>


    <p class="site-author-name" itemprop="name">关注我哟</p>
    <img class="site-author-image" itemprop="image" alt="微信公众号"
      src="/images/xlzblog.jpg">
    <div class="site-description" itemprop="description">关注公众号，更多精彩等着你</div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-小廖子"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小廖子</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">124k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:27</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'Ed2mvWHWKlphKGAa6hId76UP-gzGzoHsz',
      appKey: '7MYwWTLCtelB1RHhedbF58S4',
      placeholder: "Just go go",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: '' || 'zh-cn',
      path: location.pathname,
      recordIP: false,
      serverURLs: ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
